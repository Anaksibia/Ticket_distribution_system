{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import os.path as path\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas.plotting import lag_plot\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({'figure.figsize': (7, 5), 'figure.dpi': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/stoloto_data.csv',\n",
    "                       sep=',',\n",
    "                       parse_dates=['date'], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A glance at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we have 30021 values for all columns, no NaN values.\n",
    "# I don't see much physical meaning in statistics for categolical features such game_code, postamt_num, ufps_num\n",
    "# Hmm, std for 'ufps_num' is 0. Let's check is it a constant\n",
    "df['ufps_num'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, it is. It's useless, let's delete it\n",
    "del df['ufps_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_codes_counts = df['game_code'].value_counts()\n",
    "# game_code have only 5 possible values. Probably we should change this feature into 5 dummy variables in the regression\n",
    "game_codes = game_codes_counts.index\n",
    "print(game_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['postamt_num'].value_counts()\n",
    "# Again, postamt_num consist of 6 groups about the same size and can be replaced with 6 dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at spreading sales by combination of postamt_num and game_code\n",
    "ops_num_game_code_sales = df.pivot_table(\n",
    "                        index='postamt_num', \n",
    "                        columns='game_code', \n",
    "                        values='sales', \n",
    "                        aggfunc=sum).fillna(0).applymap(float)\n",
    "sns.heatmap(ops_num_game_code_sales, annot=True, fmt=\".1f\", linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['circulation'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ops_num'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in theory we have 5 game_codes * 93 ops_nums = 465 time series to make s forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features correlation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(df.corr())\n",
    "plt.show()\n",
    "#We have weak correlation between circulation and game_code (-0.35) but we still can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target variable survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_sales = df.sort_values(by=['date']).groupby('date').sum()['sales']\n",
    "common_sales.plot(figsize=(8,6))\n",
    "plt.ylabel('sales')\n",
    "plt.title('Sales by weeks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see two big outlaers in the end of two last years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.sort_values(by=['date']).groupby(['game_code', 'ops_num', 'date'])['sales'].sum()\n",
    "multiindex_count = df_grouped.index.to_frame()\n",
    "temp_df = pd.DataFrame(multiindex_count[['game_code', 'ops_num', 'date']].values, columns=['game_code', 'ops_num', 'date'])\n",
    "temp_df['sales'] = df_grouped.values\n",
    "multi_ts = df_grouped.unstack([0, 1])\n",
    "multi_ts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_ts.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_lengths = pd.DataFrame(len(multi_ts.index)- multi_ts.isna().sum(), columns=['ts_length']).sort_values(by=['ts_length'])\n",
    "\n",
    "plt.hist(ts_lengths['ts_length'])\n",
    "plt.xlabel('Time series')\n",
    "plt.ylabel('Number of observations')\n",
    "plt.title('Histogram of TS lengths')\n",
    "plt.figure(figsize=(4,2.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 436 time series most of which consist of 22-50 values\n",
    "# We need to deside the minimum length of time series to predict. Let's assume it to 30\n",
    "max_number_of_nas = len(multi_ts.index) - 30\n",
    "ts_min30length = multi_ts.loc[:, (multi_ts.isna().sum(axis=0) <= max_number_of_nas)]\n",
    "ts_df = pd.DataFrame() \n",
    "for i, col in enumerate(ts_min30length.columns):\n",
    "    ts_df.insert(i, str(col[0])+'_'+str(col[1]), ts_min30length.iloc[:, i])\n",
    "ts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We left 350 time series for individual forecasting with minimum 30 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at 10 random time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_rand = ts_df.sample(10, axis=1)\n",
    "ts_rand.plot(subplots=True, layout=(5,2), figsize=(8,15))\n",
    "plt.ylabel('sales')\n",
    "plt.title('10 random TS of sales by game_code and ops_num')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lot of NaN values in our time series. We have several approaches of imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random2ts = ts_df.sample(2, axis=1, random_state=1)\n",
    "random2ts.plot(subplots=True, layout=(1,2), figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill in with 0.\n",
    "# Since there are sales by certain game type in every post office for each week so we can assume that no value means 0 sales.\n",
    "# But if it is not true and data is't complete probably we shouid choose more complicatetd approach such as Backward Fill, Linear Interpolation, Quadratic interpolation, Mean of nearest neighbors\n",
    "df_zeros = ts_df.fillna(0)\n",
    "df_zeros[random2ts.columns].plot(subplots=True, layout=(1,2), figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Forward Fill\n",
    "df_ffill = ts_df.ffill().fillna(0)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
    "df_ffill[random2ts.columns[0]].plot(subplots=True, title='Forward Fill', ax=axes[0], label='Forward Fill', color='red', style=\".-\")\n",
    "ts_df[random2ts.columns[0]].plot(subplots=True, title='Forward Fill', ax=axes[0], label='Forward Fill', color='green', style=\".-\")\n",
    "axes[0].legend([\"Forward Filled Data\", \"Available Data\"])\n",
    "df_ffill[random2ts.columns[1]].plot(subplots=True, title='Forward Fill', ax=axes[1], label='Forward Fill', color='red', style=\".-\")\n",
    "ts_df[random2ts.columns[1]].plot(subplots=True, title='Forward Fill', ax=axes[1], label='Forward Fill', color='green', style=\".-\")\n",
    "axes[1].legend([\"Forward Filled Data\", \"Available Data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Backward Fill \n",
    "df_bfill = ts_df.bfill()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
    "df_bfill[random2ts.columns[0]].plot(subplots=True, title='Backward Fill ', ax=axes[0], label='Backward Fill', color='red', style=\".-\")\n",
    "ts_df[random2ts.columns[0]].plot(subplots=True, title='Backward Fill', ax=axes[0], label='Backward Fill', color='green', style=\".-\")\n",
    "axes[0].legend([\"Backward Filled Data\", \"Available Data\"])\n",
    "df_bfill[random2ts.columns[1]].plot(subplots=True, title='Backward Fill', ax=axes[1], label='Backward Fill', color='red', style=\".-\")\n",
    "ts_df[random2ts.columns[1]].plot(subplots=True, title='Backward Fill', ax=axes[1], label='Backward Fill', color='green', style=\".-\")\n",
    "axes[1].legend([\"Backward Filled Data\", \"Available Data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output preprocessed data\n",
    "__file__ = os.getcwd()\n",
    "project_path = path.normpath(path.abspath(path.join(__file__,'../')))\n",
    "df_bfill.to_csv(path.normpath(project_path +\n",
    "                                '/data/preprocessed/bfilled_ts.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Linear Interpolation\n",
    "# df_linear = pd.DataFrame()\n",
    "# ts_df['rownum'] = np.arange(ts_df.shape[0])\n",
    "# for i, col in enumerate(ts_df.columns[:-1]):\n",
    "#     df_nona = ts_df.dropna(subset = [col])\n",
    "#     f = interp1d(df_nona['rownum'], df_nona[col])\n",
    "#     df_linear[col] = f(ts_df[ts_df.index >= ts_df[col].notna().idxmax()]['rownum'])\n",
    "# ts_df.drop(['rownum'], axis=1)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
    "# df_linear[random2ts.columns[0]].plot(subplots=True, title='Linear Interpolation', ax=axes[0], label='Linear Interpolation', color='red', style=\".-\")\n",
    "# ts_df[random2ts.columns[0]].plot(subplots=True, title='Linear Interpolation', ax=axes[0], label='Linear Interpolation', color='green', style=\".-\")\n",
    "# axes[0].legend([\"Linear Interpolated Data\", \"Available Data\"])\n",
    "# df_linear[random2ts.columns[1]].plot(subplots=True, title='Linear Interpolation', ax=axes[1], label='Linear Interpolation', color='red', style=\".-\")\n",
    "# ts_df[random2ts.columns[1]].plot(subplots=True, title='Linear Interpolation', ax=axes[1], label='Linear Interpolation', color='green', style=\".-\")\n",
    "# axes[1].legend([\"Linear Interpolated Data\", \"Available Data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Mean of 'n' Nearest Past Neighbors\n",
    "def knn_mean(ts, n):\n",
    "    out = np.copy(ts)\n",
    "    for i, val in enumerate(ts):\n",
    "        if np.isnan(val):\n",
    "            n_by_2 = np.ceil(n/2)\n",
    "            lower = np.max([0, int(i-n_by_2)])\n",
    "            upper = np.min([len(ts)+1, int(i+n_by_2)])\n",
    "            ts_near = np.concatenate([ts[lower:i], ts[i:upper]])\n",
    "            out[i] = np.nanmean(ts_near)\n",
    "    return out\n",
    "\n",
    "df_knn = ts_df.copy()\n",
    "for col in df_knn.columns:\n",
    "    while df_knn[col].isnull().sum() > 0:\n",
    "        df_knn[col] = knn_mean(df_knn[col].values, 8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "df_knn[random2ts.columns[0]].plot(subplots=True, title='kNN', ax=axes[0], label='kNN', color='red', style=\".-\")\n",
    "ts_df[random2ts.columns[0]].plot(subplots=True, title='kNN', ax=axes[0], label='kNN', color='green', style=\".-\")\n",
    "axes[0].legend([\"Mean of 'n' Nearest Past Neighbors\", \"Available Data\"])\n",
    "df_knn[random2ts.columns[1]].plot(subplots=True, title='kNN', ax=axes[1], label='kNN', color='red', style=\".-\")\n",
    "ts_df[random2ts.columns[1]].plot(subplots=True, title='kNN', ax=axes[1], label='kNN', color='green', style=\".-\")\n",
    "axes[1].legend([\"Mean of 'n' Nearest Past Neighbors\", \"Available Data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output preprocessed data\n",
    "__file__ = os.getcwd()\n",
    "project_path = path.normpath(path.abspath(path.join(__file__,'../')))\n",
    "df_knn.to_csv(path.normpath(project_path +\n",
    "                                '/data/preprocessed/knn_imputated_ts.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that TS with different game_code may differ from each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game code TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_code_df = temp_df.groupby('game_code').sum()\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.ylabel('sales')\n",
    "plt.title('Sales by game_code')\n",
    "plt.plot(game_code_df, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales value sugnificatly differ from game type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_code_ts = temp_df.groupby(['game_code', 'date']).sum()\n",
    "ts_by_game_code = game_code_ts.unstack([0])\n",
    "#ts_by_game_code.droplevel(0, axis=1)\n",
    "ts_by_game_code.columns = [col[1] for col in ts_by_game_code.columns]\n",
    "ts_by_game_code.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TS have too strong outliers. Let's remove it to see other parts of plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(ts_by_game_code.iloc[:, [i]].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "game_code_no_outlier = ts_by_game_code.drop([datetime.date(2017,12,31), datetime.date(2018,12,30)])\n",
    "\n",
    "plt.ylabel('sales')\n",
    "plt.title('Time Series of Sales by game_code')\n",
    "plt.plot(game_code_no_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see similarity at time series trends. Let's investidate it by trend and seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend and Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of Month-wise (Seasonal) and Year-wise (trend) Distribution\n",
    "# Prepare data\n",
    "#ts_by_game_code.rename(columns = {7101: '7101', 7103: '7103', 7105: '7105', 7115: '7115', 7175: '7175'}, inplace = True)\n",
    "ts_by_years = ts_by_game_code.reset_index(inplace=False)\n",
    "\n",
    "ts_by_years['year'] = [d.year for d in ts_by_years.date]\n",
    "ts_by_years['month'] = [d.strftime('%b') for d in ts_by_years.date]\n",
    "years = ts_by_years['year'].unique()\n",
    "# Plot\n",
    "for i, game_code in enumerate(game_codes):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10,10), dpi= 80)\n",
    "    year_box = sns.boxplot(x='year', y=game_code, data=ts_by_years, ax=axes[0])\n",
    "    month_box = sns.boxplot(x='month', y=game_code, data=ts_by_years)\n",
    "    upper_whisker_limit = ts_by_years[game_code].quantile(0.75) + 1.6 * (ts_by_years[game_code].quantile(0.75) - ts_by_years[game_code].quantile(0.25))\n",
    "    year_box.set(ylim=(1, upper_whisker_limit))\n",
    "    month_box.set(ylim=(1,  upper_whisker_limit))\n",
    "    # Set Title\n",
    "    axes[0].set_title('Year-wise Box Plot\\n(The Trend)\\n' + str(game_code), fontsize=18); \n",
    "    axes[1].set_title('Month-wise Box Plot\\n(The Seasonality)\\n' + str(game_code), fontsize=18)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first 3 time series (game types 7103, 7105 and 7115) we see strong uprising trend that testifies to stationarity.\n",
    "We can see some seasonality - sales are increasing from May to October.\n",
    "Also we can see several outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polar plots\n",
    "mounthly_ts = ts_by_years.groupby('month').sum()\n",
    "for i, game_code in enumerate(game_codes):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(5,5), dpi= 80)\n",
    "    plt.title(str(game_code))\n",
    "    plt.polar(mounthly_ts.iloc[:, i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a high peak in December and slight tendency to increased sales in cold months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trend, Season Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand5ts =  df_bfill.sample(5, axis=1, random_state=1)\n",
    "for i, col in enumerate(rand5ts.columns):\n",
    "    # Multiplicative Decomposition \n",
    "    #result_mul = seasonal_decompose(ts_rand_0.iloc[:, i], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "    # Additive Decomposition\n",
    "    result_add = seasonal_decompose(rand5ts.iloc[:, i], model='additive', extrapolate_trend='freq')\n",
    "\n",
    "    # Plot\n",
    "    plt.rcParams.update({'figure.figsize': (8,8)})\n",
    "    #result_mul.plot().suptitle('Multiplicative Decompose' + str(col), fontsize=22)\n",
    "    result_add.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examinate on outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore on outliers common_sales\n",
    "\n",
    "# import sesd\n",
    "\n",
    "# outliers_indices = sesd.seasonal_esd(common_sales, hybrid=True, max_anomalies=5)\n",
    "# for idx in outliers_indices:\n",
    "#     print(f'Anomaly index: {idx}, anomaly value: {common_sales[idx]}')\n",
    "# print('--------\\n')\n",
    "\n",
    "# # And compare it with single TS\n",
    "# for i, col in enumerate(rand5ts.columns):\n",
    "#     anomaly_index = sesd.seasonal_esd(rand5ts.iloc[:, i], hybrid=True, max_anomalies=5)\n",
    "#     for idx in anomaly_index:\n",
    "#         print(f'Anomaly index: {idx}, anomaly value: {rand5ts.iloc[idx, i]}')\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found only 4 outliers in time series of all sales. Found indexes are also occures in particular time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing data\n",
    "\n",
    "rolled_knn = pd.DataFrame() \n",
    "for i, col in enumerate(df_knn.columns):\n",
    "    rolled_knn.insert(i, col, df_knn.iloc[:, i].rolling(window=4).mean(), True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "rolled_knn[random2ts.columns[0]].plot(subplots=True, title='Rolling mean trend', ax=axes[0], label='Rolling mean trend', color='green')\n",
    "df_knn[random2ts.columns[0]].plot(subplots=True, title='', ax=axes[0], label='Rolling mean trend', color='blue')\n",
    "axes[0].legend([\"Rolling mean trend\", \"Actual Data\"])\n",
    "rolled_knn[random2ts.columns[1]].plot(subplots=True, title='Rolling mean trend', ax=axes[1], label='Rolling mean trend', color='green')\n",
    "df_knn[random2ts.columns[1]].plot(subplots=True, title='Rolling mean trend', ax=axes[1], label='Rolling mean trend', color='blue')\n",
    "axes[1].legend([\"Rolling mean trend\", \"Actual Data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF test\n",
    "def adf_nonstationarity_counter(df):\n",
    "    nonstationarity_count = 0\n",
    "    for i, col in enumerate(df.columns):\n",
    "        test = adfuller(df.iloc[:, i])\n",
    "        if test[0]> test[4]['5%']: \n",
    "            nonstationarity_count +=1\n",
    "    return('Nonstationary ts number: ' + str(nonstationarity_count) + '\\nShare of all TS: '+ str(nonstationarity_count/df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial state\n",
    "print(adf_nonstationarity_counter(df_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68 of 350 time series (19,4%) are non-stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using statmodels: Subtracting the Trend Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended = pd.DataFrame() \n",
    "for i, col in enumerate(df_knn.columns):\n",
    "    # Additive Decomposition\n",
    "    result_add = seasonal_decompose(df_knn.iloc[:, i], model='additive', extrapolate_trend='freq')\n",
    "    detrended.insert(i, col, (df_knn.iloc[:, i].values - result_add.trend)/ result_add.seasonal, True)\n",
    "    \n",
    "print(adf_nonstationarity_counter(detrended))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differencing Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1diff = df_zeros.diff(periods=1).dropna()\n",
    "print(adf_nonstationarity_counter(ts1diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencing TS shows better results. Only 1.1% of TS appears to be nonstationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1diff_knn = df_knn.diff(periods=1).dropna()\n",
    "print(adf_nonstationarity_counter(ts1diff_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_rand_1diff = ts1diff.sample(4, axis=1)\n",
    "ts_rand_1diff.plot(subplots=True, layout=(2,2), figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2diff = df_knn.diff(periods=2).dropna()\n",
    "print(adf_nonstationarity_counter(ts2diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencing 2 times is worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Lag Scatter Plots\n",
    "\n",
    "rand4ts = df_knn.sample(4, axis=1)\n",
    "#c1, c2, c3, c4 = sns.color_palette(\"husl\", 4)\n",
    "ax1 = plt.subplot2grid((2,2), (0,0))\n",
    "ax2 = plt.subplot2grid((2,2), (0,1))\n",
    "ax3 = plt.subplot2grid((2,2), (1,0))\n",
    "ax4 = plt.subplot2grid((2,2), (1,1))\n",
    "\n",
    "lag_plot(df_knn[rand4ts.columns[0]], ax=ax1, alpha=0.5)\n",
    "lag_plot(df_knn[rand4ts.columns[1]], ax=ax2, alpha=0.5)\n",
    "lag_plot(df_knn[rand4ts.columns[2]], ax=ax3, alpha=0.5)\n",
    "lag_plot(df_knn[rand4ts.columns[3]], ax=ax4, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoll (regular) values are correlated with their previous value. Large values are not, there are outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation plots\n",
    "\n",
    "ax1 = plt.subplot2grid((2,2), (0,0))\n",
    "ax2 = plt.subplot2grid((2,2), (0,1))\n",
    "ax3 = plt.subplot2grid((2,2), (1,0))\n",
    "ax4 = plt.subplot2grid((2,2), (1,1))\n",
    "\n",
    "autocorrelation_plot(df_knn[rand4ts.columns[0]], ax=ax1)\n",
    "autocorrelation_plot(df_knn[rand4ts.columns[1]], ax=ax2)\n",
    "autocorrelation_plot(df_knn[rand4ts.columns[2]], ax=ax3)\n",
    "autocorrelation_plot(df_knn[rand4ts.columns[3]], ax=ax4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see autocorrelation with several first lags. And there is a peak between two holiday pre-New Year weeks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
